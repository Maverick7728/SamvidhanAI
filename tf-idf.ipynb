{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5030c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99778c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c701c331a8546cda5b2918808fe1e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/85.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\manan\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\manan\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "     --------------------------------------- 11.6/11.6 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openpyxl in c:\\users\\manan\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2023.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\manan\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\manan\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\manan\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas.libs\\\\msvcp140-fa0758dedafbbe194d3ee96e3dc2b9a3.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866bcb4b04a149b8a8d587dac4307e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.csv:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afbd5de02954aaf9f81ab32e7de04cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7bb264c98a49bcb818262286c577cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a674cdf6af64418c9c7fe87d94ab73a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27c31e9499e4086abc694e773371901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manan\\AppData\\Local\\Temp\\ipykernel_12484\\1369951202.py:11: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  df.to_excel(\"casehold_train.xlsx\", index=False)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with trust_remote_code\n",
    "dataset = load_dataset(\"casehold/casehold\", trust_remote_code=True)\n",
    "\n",
    "# Convert the training data to a DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel(\"casehold_train.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23f15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9853eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f42e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\manan\\AppData\\Local\\Temp\\ipykernel_12484\\4131426476.py:50: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  processed_df.to_excel(\"casehold_citing_prompt_preprocessed.xlsx\", index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"casehold_train.xlsx\")\n",
    "\n",
    "# Preprocessing functions\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "def lowercase_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token.isalnum()]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens_lower = lowercase_tokens(tokens)\n",
    "    tokens_no_stop = remove_stopwords(tokens_lower)\n",
    "    tokens_no_punct = remove_punctuation(tokens_no_stop)\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokenized': tokens,\n",
    "        'lowercase': tokens_lower,\n",
    "        'no_stopwords': tokens_no_stop,\n",
    "        'no_punctuation': tokens_no_punct\n",
    "    }\n",
    "\n",
    "# Apply to 'citing_prompt' column\n",
    "processed = df['citing_prompt'].apply(preprocess_text)\n",
    "\n",
    "# Normalize to new DataFrame\n",
    "processed_df = pd.json_normalize(processed)\n",
    "\n",
    "# Optional: Add example_id back for reference\n",
    "processed_df['example_id'] = df['example_id']\n",
    "\n",
    "# Save the result\n",
    "processed_df.to_excel(\"casehold_citing_prompt_preprocessed.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "627f348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT (truncated):\n",
      "Colameta used customer information that he took from ProtÃ©gÃ©. Additionally, Colameta admits to having taken at least two ProtÃ©gÃ© proposals with him to Monument. This type of information may constitute trade secrets. See G.L.c. 266, Â§30 (defining â€œtrade secretâ€ as used in G.L.c. 93, Â§42, as including â€œanything tangible or intangible or electronically kept or stored, which constitutes, represents, evidences or records a secret scientific, technical, merchandising, production or management informat...\n",
      "\n",
      "STEP 1: TOKENIZATION (first 78 tokens)\n",
      "['Colameta', 'used', 'customer', 'information', 'that', 'he', 'took', 'from', 'ProtÃ©gÃ©', '.', 'Additionally', ',', 'Colameta', 'admits', 'to', 'having', 'taken', 'at', 'least', 'two', 'ProtÃ©gÃ©', 'proposals', 'with', 'him', 'to', 'Monument', '.', 'This', 'type', 'of', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'See', 'G.L.c', '.', '266', ',', 'Â§30', '(', 'defining', 'â€œ', 'trade', 'secret', 'â€', 'as', 'used', 'in', 'G.L.c', '.', '93', ',', 'Â§42', ',', 'as', 'including', 'â€œ', 'anything', 'tangible', 'or', 'intangible', 'or', 'electronically', 'kept', 'or', 'stored', ',', 'which', 'constitutes', ',', 'represents', ',', 'evidences', 'or', 'records']\n",
      "Total tokens: 167\n",
      "\n",
      "STEP 2: LOWERCASE (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'that', 'he', 'took', 'from', 'protÃ©gÃ©', '.', 'additionally', ',', 'colameta', 'admits', 'to', 'having', 'taken', 'at', 'least', 'two', 'protÃ©gÃ©', 'proposals', 'with', 'him', 'to', 'monument', '.', 'this', 'type', 'of', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'see', 'g.l.c', '.', '266', ',', 'Â§30', '(', 'defining', 'â€œ', 'trade', 'secret', 'â€', 'as', 'used', 'in', 'g.l.c', '.', '93', ',', 'Â§42', ',', 'as', 'including', 'â€œ', 'anything', 'tangible', 'or', 'intangible', 'or', 'electronically', 'kept', 'or', 'stored', ',', 'which', 'constitutes', ',', 'represents', ',', 'evidences', 'or', 'records']\n",
      "Total tokens: 167\n",
      "\n",
      "STEP 3: STOPWORD REMOVAL (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'took', 'protÃ©gÃ©', '.', 'additionally', ',', 'colameta', 'admits', 'taken', 'least', 'two', 'protÃ©gÃ©', 'proposals', 'monument', '.', 'type', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'see', 'g.l.c', '.', '266', ',', 'Â§30', '(', 'defining', 'â€œ', 'trade', 'secret', 'â€', 'used', 'g.l.c', '.', '93', ',', 'Â§42', ',', 'including', 'â€œ', 'anything', 'tangible', 'intangible', 'electronically', 'kept', 'stored', ',', 'constitutes', ',', 'represents', ',', 'evidences', 'records', 'secret', 'scientific', ',', 'technical', ',', 'merchandising', ',', 'production', 'management', 'information', ',', 'design', ',', 'process', ',', 'procedure', ',', 'formula', ',']\n",
      "Total tokens: 133\n",
      "\n",
      "STEP 4: PUNCTUATION REMOVAL (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'took', 'protÃ©gÃ©', 'additionally', 'colameta', 'admits', 'taken', 'least', 'two', 'protÃ©gÃ©', 'proposals', 'monument', 'type', 'information', 'may', 'constitute', 'trade', 'secrets', 'see', '266', 'defining', 'trade', 'secret', 'used', '93', 'including', 'anything', 'tangible', 'intangible', 'electronically', 'kept', 'stored', 'constitutes', 'represents', 'evidences', 'records', 'secret', 'scientific', 'technical', 'merchandising', 'production', 'management', 'information', 'design', 'process', 'procedure', 'formula', 'invention', 'improvement', '427', 'mass', '49', 'confidential', 'proprietary', 'business', 'information', 'may', 'entitled', 'protection', 'even', 'information', 'claim', 'trade', 'secret', 'protection', 'see', 'augat', '409', 'mass', '173', 'holding', 'matters', 'public', 'knowledge', 'general']\n",
      "Total tokens: 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed citing_prompt data\n",
    "preprocessed_samples = pd.read_excel(\"casehold_citing_prompt_preprocessed.xlsx\")\n",
    "\n",
    "# Convert string representations of lists back into actual lists (Excel saves them as strings)\n",
    "import ast\n",
    "\n",
    "for col in ['tokenized', 'lowercase', 'no_stopwords', 'no_punctuation']:\n",
    "    preprocessed_samples[col] = preprocessed_samples[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Focus on Sample 1 for detailed analysis\n",
    "sample_idx = 1\n",
    "sample = preprocessed_samples.iloc[sample_idx]\n",
    "\n",
    "# Original text\n",
    "print(f\"ORIGINAL TEXT (truncated):\\n{sample['original'][:500]}...\\n\")\n",
    "\n",
    "# Step 1: Tokenization\n",
    "print(\"STEP 1: TOKENIZATION (first 78 tokens)\")\n",
    "print(sample['tokenized'][:78])\n",
    "print(f\"Total tokens: {len(sample['tokenized'])}\\n\")\n",
    "\n",
    "# Step 2: Lowercase\n",
    "print(\"STEP 2: LOWERCASE (first 78 tokens)\")\n",
    "print(sample['lowercase'][:78])\n",
    "print(f\"Total tokens: {len(sample['lowercase'])}\\n\")\n",
    "\n",
    "# Step 3: Stopword Removal\n",
    "print(\"STEP 3: STOPWORD REMOVAL (first 78 tokens)\")\n",
    "print(sample['no_stopwords'][:78])\n",
    "print(f\"Total tokens: {len(sample['no_stopwords'])}\\n\")\n",
    "\n",
    "# Step 4: Punctuation Removal\n",
    "print(\"STEP 4: PUNCTUATION REMOVAL (first 78 tokens)\")\n",
    "print(sample['no_punctuation'][:78])\n",
    "print(f\"Total tokens: {len(sample['no_punctuation'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5afd137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Available columns:\n",
      " ['example_id', 'citing_prompt', 'holding_0', 'holding_1', 'holding_2', 'holding_3', 'holding_4']\n",
      "âœ… Matched Columns:\n",
      "Example ID Column: example_id\n",
      "Citing Prompt Column: citing_prompt\n",
      "Holding Columns: ['holding_0', 'holding_1', 'holding_2', 'holding_3', 'holding_4']\n",
      "\n",
      "ðŸ“Œ Best Holding Result:\n",
      "{'example_id': 3, 'best_holding': 'holding_0', 'similarity_score': 0.1123, 'holding_text': 'holding that plaintiff stated a  349 claim where plaintiff alleged facts plausibly suggesting that defendant intentionally registered its corporate name to be confusingly similar to plaintiffs commscope trademark'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(\"casehold_train.xlsx\")\n",
    "\n",
    "# Strip whitespace and lower all column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Print all column names to debug\n",
    "print(\"âœ… Available columns:\\n\", df.columns.tolist())\n",
    "\n",
    "# Define best matching column function\n",
    "def get_best_column_match(possible_names):\n",
    "    for name in possible_names:\n",
    "        for col in df.columns:\n",
    "            if name.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# Try to find correct column names\n",
    "example_id_col = get_best_column_match([\"example_id\"])\n",
    "citing_prompt_col = get_best_column_match([\"citing_prompt\"])\n",
    "holding_cols = [get_best_column_match([f\"holding_{i}\"]) for i in range(5)]\n",
    "\n",
    "# Check if any column is missing\n",
    "if not all([example_id_col, citing_prompt_col] + holding_cols):\n",
    "    print(\"âŒ Error: One or more required columns not found.\")\n",
    "    print(\"ðŸ” Detected:\", example_id_col, citing_prompt_col, holding_cols)\n",
    "else:\n",
    "    print(\"âœ… Matched Columns:\")\n",
    "    print(f\"Example ID Column: {example_id_col}\")\n",
    "    print(f\"Citing Prompt Column: {citing_prompt_col}\")\n",
    "    print(f\"Holding Columns: {holding_cols}\")\n",
    "\n",
    "    # TF-IDF Logic\n",
    "    def find_best_holding(example_id):\n",
    "        row = df[df[example_id_col] == example_id]\n",
    "\n",
    "        if row.empty:\n",
    "            return f\"âŒ No data found for example_id: {example_id}\"\n",
    "\n",
    "        citing_prompt = row[citing_prompt_col].values[0]\n",
    "        holdings = [row[col].values[0] for col in holding_cols]\n",
    "\n",
    "        corpus = [citing_prompt] + holdings\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "        similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        best_index = similarities.argmax()\n",
    "        best_score = similarities[best_index]\n",
    "        best_holding_col = holding_cols[best_index]\n",
    "\n",
    "        return {\n",
    "            'example_id': example_id,\n",
    "            'best_holding': best_holding_col,\n",
    "            'similarity_score': round(float(best_score), 4),\n",
    "            'holding_text': holdings[best_index]\n",
    "        }\n",
    "\n",
    "    # Try it!\n",
    "    example_id_input = 3\n",
    "    result = find_best_holding(example_id_input)\n",
    "    print(\"\\nðŸ“Œ Best Holding Result:\")\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
