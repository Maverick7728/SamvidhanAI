{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5030c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99778c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c701c331a8546cda5b2918808fe1e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/85.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\manan\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\manan\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "     --------------------------------------- 11.6/11.6 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openpyxl in c:\\users\\manan\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2023.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\manan\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manan\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\manan\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\manan\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas.libs\\\\msvcp140-fa0758dedafbbe194d3ee96e3dc2b9a3.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\manan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866bcb4b04a149b8a8d587dac4307e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.csv:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afbd5de02954aaf9f81ab32e7de04cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7bb264c98a49bcb818262286c577cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a674cdf6af64418c9c7fe87d94ab73a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27c31e9499e4086abc694e773371901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manan\\AppData\\Local\\Temp\\ipykernel_12484\\1369951202.py:11: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  df.to_excel(\"casehold_train.xlsx\", index=False)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with trust_remote_code\n",
    "dataset = load_dataset(\"casehold/casehold\", trust_remote_code=True)\n",
    "\n",
    "# Convert the training data to a DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel(\"casehold_train.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23f15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9853eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f42e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\manan\\AppData\\Local\\Temp\\ipykernel_12484\\4131426476.py:50: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  processed_df.to_excel(\"casehold_citing_prompt_preprocessed.xlsx\", index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"casehold_train.xlsx\")\n",
    "\n",
    "# Preprocessing functions\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "def lowercase_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token.isalnum()]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens_lower = lowercase_tokens(tokens)\n",
    "    tokens_no_stop = remove_stopwords(tokens_lower)\n",
    "    tokens_no_punct = remove_punctuation(tokens_no_stop)\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokenized': tokens,\n",
    "        'lowercase': tokens_lower,\n",
    "        'no_stopwords': tokens_no_stop,\n",
    "        'no_punctuation': tokens_no_punct\n",
    "    }\n",
    "\n",
    "# Apply to 'citing_prompt' column\n",
    "processed = df['citing_prompt'].apply(preprocess_text)\n",
    "\n",
    "# Normalize to new DataFrame\n",
    "processed_df = pd.json_normalize(processed)\n",
    "\n",
    "# Optional: Add example_id back for reference\n",
    "processed_df['example_id'] = df['example_id']\n",
    "\n",
    "# Save the result\n",
    "processed_df.to_excel(\"casehold_citing_prompt_preprocessed.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "627f348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT (truncated):\n",
      "Colameta used customer information that he took from Protégé. Additionally, Colameta admits to having taken at least two Protégé proposals with him to Monument. This type of information may constitute trade secrets. See G.L.c. 266, §30 (defining “trade secret” as used in G.L.c. 93, §42, as including “anything tangible or intangible or electronically kept or stored, which constitutes, represents, evidences or records a secret scientific, technical, merchandising, production or management informat...\n",
      "\n",
      "STEP 1: TOKENIZATION (first 78 tokens)\n",
      "['Colameta', 'used', 'customer', 'information', 'that', 'he', 'took', 'from', 'Protégé', '.', 'Additionally', ',', 'Colameta', 'admits', 'to', 'having', 'taken', 'at', 'least', 'two', 'Protégé', 'proposals', 'with', 'him', 'to', 'Monument', '.', 'This', 'type', 'of', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'See', 'G.L.c', '.', '266', ',', '§30', '(', 'defining', '“', 'trade', 'secret', '”', 'as', 'used', 'in', 'G.L.c', '.', '93', ',', '§42', ',', 'as', 'including', '“', 'anything', 'tangible', 'or', 'intangible', 'or', 'electronically', 'kept', 'or', 'stored', ',', 'which', 'constitutes', ',', 'represents', ',', 'evidences', 'or', 'records']\n",
      "Total tokens: 167\n",
      "\n",
      "STEP 2: LOWERCASE (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'that', 'he', 'took', 'from', 'protégé', '.', 'additionally', ',', 'colameta', 'admits', 'to', 'having', 'taken', 'at', 'least', 'two', 'protégé', 'proposals', 'with', 'him', 'to', 'monument', '.', 'this', 'type', 'of', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'see', 'g.l.c', '.', '266', ',', '§30', '(', 'defining', '“', 'trade', 'secret', '”', 'as', 'used', 'in', 'g.l.c', '.', '93', ',', '§42', ',', 'as', 'including', '“', 'anything', 'tangible', 'or', 'intangible', 'or', 'electronically', 'kept', 'or', 'stored', ',', 'which', 'constitutes', ',', 'represents', ',', 'evidences', 'or', 'records']\n",
      "Total tokens: 167\n",
      "\n",
      "STEP 3: STOPWORD REMOVAL (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'took', 'protégé', '.', 'additionally', ',', 'colameta', 'admits', 'taken', 'least', 'two', 'protégé', 'proposals', 'monument', '.', 'type', 'information', 'may', 'constitute', 'trade', 'secrets', '.', 'see', 'g.l.c', '.', '266', ',', '§30', '(', 'defining', '“', 'trade', 'secret', '”', 'used', 'g.l.c', '.', '93', ',', '§42', ',', 'including', '“', 'anything', 'tangible', 'intangible', 'electronically', 'kept', 'stored', ',', 'constitutes', ',', 'represents', ',', 'evidences', 'records', 'secret', 'scientific', ',', 'technical', ',', 'merchandising', ',', 'production', 'management', 'information', ',', 'design', ',', 'process', ',', 'procedure', ',', 'formula', ',']\n",
      "Total tokens: 133\n",
      "\n",
      "STEP 4: PUNCTUATION REMOVAL (first 78 tokens)\n",
      "['colameta', 'used', 'customer', 'information', 'took', 'protégé', 'additionally', 'colameta', 'admits', 'taken', 'least', 'two', 'protégé', 'proposals', 'monument', 'type', 'information', 'may', 'constitute', 'trade', 'secrets', 'see', '266', 'defining', 'trade', 'secret', 'used', '93', 'including', 'anything', 'tangible', 'intangible', 'electronically', 'kept', 'stored', 'constitutes', 'represents', 'evidences', 'records', 'secret', 'scientific', 'technical', 'merchandising', 'production', 'management', 'information', 'design', 'process', 'procedure', 'formula', 'invention', 'improvement', '427', 'mass', '49', 'confidential', 'proprietary', 'business', 'information', 'may', 'entitled', 'protection', 'even', 'information', 'claim', 'trade', 'secret', 'protection', 'see', 'augat', '409', 'mass', '173', 'holding', 'matters', 'public', 'knowledge', 'general']\n",
      "Total tokens: 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed citing_prompt data\n",
    "preprocessed_samples = pd.read_excel(\"casehold_citing_prompt_preprocessed.xlsx\")\n",
    "\n",
    "# Convert string representations of lists back into actual lists (Excel saves them as strings)\n",
    "import ast\n",
    "\n",
    "for col in ['tokenized', 'lowercase', 'no_stopwords', 'no_punctuation']:\n",
    "    preprocessed_samples[col] = preprocessed_samples[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Focus on Sample 1 for detailed analysis\n",
    "sample_idx = 1\n",
    "sample = preprocessed_samples.iloc[sample_idx]\n",
    "\n",
    "# Original text\n",
    "print(f\"ORIGINAL TEXT (truncated):\\n{sample['original'][:500]}...\\n\")\n",
    "\n",
    "# Step 1: Tokenization\n",
    "print(\"STEP 1: TOKENIZATION (first 78 tokens)\")\n",
    "print(sample['tokenized'][:78])\n",
    "print(f\"Total tokens: {len(sample['tokenized'])}\\n\")\n",
    "\n",
    "# Step 2: Lowercase\n",
    "print(\"STEP 2: LOWERCASE (first 78 tokens)\")\n",
    "print(sample['lowercase'][:78])\n",
    "print(f\"Total tokens: {len(sample['lowercase'])}\\n\")\n",
    "\n",
    "# Step 3: Stopword Removal\n",
    "print(\"STEP 3: STOPWORD REMOVAL (first 78 tokens)\")\n",
    "print(sample['no_stopwords'][:78])\n",
    "print(f\"Total tokens: {len(sample['no_stopwords'])}\\n\")\n",
    "\n",
    "# Step 4: Punctuation Removal\n",
    "print(\"STEP 4: PUNCTUATION REMOVAL (first 78 tokens)\")\n",
    "print(sample['no_punctuation'][:78])\n",
    "print(f\"Total tokens: {len(sample['no_punctuation'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5afd137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Available columns:\n",
      " ['example_id', 'citing_prompt', 'holding_0', 'holding_1', 'holding_2', 'holding_3', 'holding_4']\n",
      "✅ Matched Columns:\n",
      "Example ID Column: example_id\n",
      "Citing Prompt Column: citing_prompt\n",
      "Holding Columns: ['holding_0', 'holding_1', 'holding_2', 'holding_3', 'holding_4']\n",
      "\n",
      "📌 Best Holding Result:\n",
      "{'example_id': 3, 'best_holding': 'holding_0', 'similarity_score': 0.1123, 'holding_text': 'holding that plaintiff stated a  349 claim where plaintiff alleged facts plausibly suggesting that defendant intentionally registered its corporate name to be confusingly similar to plaintiffs commscope trademark'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(\"casehold_train.xlsx\")\n",
    "\n",
    "# Strip whitespace and lower all column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Print all column names to debug\n",
    "print(\"✅ Available columns:\\n\", df.columns.tolist())\n",
    "\n",
    "# Define best matching column function\n",
    "def get_best_column_match(possible_names):\n",
    "    for name in possible_names:\n",
    "        for col in df.columns:\n",
    "            if name.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# Try to find correct column names\n",
    "example_id_col = get_best_column_match([\"example_id\"])\n",
    "citing_prompt_col = get_best_column_match([\"citing_prompt\"])\n",
    "holding_cols = [get_best_column_match([f\"holding_{i}\"]) for i in range(5)]\n",
    "\n",
    "# Check if any column is missing\n",
    "if not all([example_id_col, citing_prompt_col] + holding_cols):\n",
    "    print(\"❌ Error: One or more required columns not found.\")\n",
    "    print(\"🔍 Detected:\", example_id_col, citing_prompt_col, holding_cols)\n",
    "else:\n",
    "    print(\"✅ Matched Columns:\")\n",
    "    print(f\"Example ID Column: {example_id_col}\")\n",
    "    print(f\"Citing Prompt Column: {citing_prompt_col}\")\n",
    "    print(f\"Holding Columns: {holding_cols}\")\n",
    "\n",
    "    # TF-IDF Logic\n",
    "    def find_best_holding(example_id):\n",
    "        row = df[df[example_id_col] == example_id]\n",
    "\n",
    "        if row.empty:\n",
    "            return f\"❌ No data found for example_id: {example_id}\"\n",
    "\n",
    "        citing_prompt = row[citing_prompt_col].values[0]\n",
    "        holdings = [row[col].values[0] for col in holding_cols]\n",
    "\n",
    "        corpus = [citing_prompt] + holdings\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "        similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        best_index = similarities.argmax()\n",
    "        best_score = similarities[best_index]\n",
    "        best_holding_col = holding_cols[best_index]\n",
    "\n",
    "        return {\n",
    "            'example_id': example_id,\n",
    "            'best_holding': best_holding_col,\n",
    "            'similarity_score': round(float(best_score), 4),\n",
    "            'holding_text': holdings[best_index]\n",
    "        }\n",
    "\n",
    "    # Try it!\n",
    "    example_id_input = 3\n",
    "    result = find_best_holding(example_id_input)\n",
    "    print(\"\\n📌 Best Holding Result:\")\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
